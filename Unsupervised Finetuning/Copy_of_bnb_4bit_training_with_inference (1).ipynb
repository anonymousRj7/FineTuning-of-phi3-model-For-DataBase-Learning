{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps \"xformers<0.0.26\" trl peft accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "tENYnvYnAHfa"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/mistral-7b-bnb-4bit\",\n",
        "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
        "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
        "    \"unsloth/gemma-7b-bnb-4bit\",\n",
        "    \"unsloth/gemma-7b-it-bnb-4bit\", # Instruct version of Gemma 7b\n",
        "    \"unsloth/gemma-2b-bnb-4bit\",\n",
        "    \"unsloth/gemma-2b-it-bnb-4bit\", # Instruct version of Gemma 2b\n",
        "    \"unsloth/llama-3-8b-bnb-4bit\", # [NEW] 15 Trillion token Llama-3\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Phi-3-mini-4k-instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ],
      "metadata": {
        "id": "E0Nl5mWL0k2T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62012936-c4db-4829-98ab-97a95f5e984e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth: Fast Mistral patching release 2024.5\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.2.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. Xformers = 0.0.25.post1. FA = False.\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we have to apply some preprocessing to the model to prepare it for training. For that use the `prepare_model_for_kbit_training` method from PEFT."
      ],
      "metadata": {
        "id": "Mp2gMi1ZzGET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)"
      ],
      "metadata": {
        "id": "a9EUEDAl0ss3"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "gkIcwsSU01EB"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "data = load_dataset(\"rajanonymous12/info_about_database_table_relation\")"
      ],
      "metadata": {
        "id": "s6f4z8EYmcJ6"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.map(lambda samples: tokenizer(samples[\"input\"]), batched=True)"
      ],
      "metadata": {
        "id": "MVCvNWDJCuGk"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "6QtNzN2tA-mg",
        "outputId": "d59ae239-5a66-4795-becd-106d2827cab4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input', 'input_ids', 'attention_mask'],\n",
              "        num_rows: 91\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 128, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ],
      "metadata": {
        "id": "wyKc1ExzCF1c"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\n",
        "# needed for gpt-neo-x tokenizer\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "# Check if the tokenizer has a mask token\n",
        "# if tokenizer.mask_token is None:\n",
        "#     # Add a mask token\n",
        "#     tokenizer.add_special_tokens({'mask_token': '[MASK]'})\n",
        "\n",
        "\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=data[\"train\"],\n",
        "    args=transformers.TrainingArguments(\n",
        "        # num_train_epochs=20,\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=2,\n",
        "        max_steps=10,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=True,\n",
        "        logging_steps=1,\n",
        "        output_dir=\"outputs\",\n",
        "        optim=\"paged_adamw_8bit\"\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "id": "jq0nX33BmfaC",
        "outputId": "6eb90e69-be7a-4c13-bb56-ebaf8ab5c71c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 91 | Num Epochs = 1\n",
            "O^O/ \\_/ \\    Batch size per device = 1 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 4 | Total steps = 10\n",
            " \"-____-\"     Number of trainable parameters = 239,075,328\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 00:47, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.713300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.724800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.676200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.434100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.336000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.005200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.884500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.857600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.819000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.122700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=10, training_loss=1.3573333084583283, metrics={'train_runtime': 54.0094, 'train_samples_per_second': 0.741, 'train_steps_per_second': 0.185, 'total_flos': 476705858549760.0, 'train_loss': 1.3573333084583283, 'epoch': 0.43956043956043955})"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Output 3"
      ],
      "metadata": {
        "id": "hN6i0KurhCgF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"You have to answer the question only in json format.For example if user ask : Give me top 5 customer with most number of orders.\n",
        "Then you have to give response as :\n",
        "[ \"table_information\": [\n",
        "{\n",
        "\"table_name\": \"customer_0\",\n",
        "\"table_description\": \"Stores detailed customer information and behavior\",\n",
        "\"primary_key_column\": [\n",
        "\"customer_id\"\n",
        "],\n",
        "\"columns\": [\n",
        "{\n",
        "\"name\": \"customer_id\",\n",
        "\"description\": \"Unique identifier for the customer\",\n",
        "\"data_type\": \"STRING\",\n",
        "\"format\": \"\",\n",
        "\"is_pii_column\": \"Y\",\n",
        "\"enum\": [],\n",
        "\"dimension_group\": \"\",\n",
        "\"is_nullable\": \"N\"\n",
        "},\n",
        "{\n",
        "\"name\": \"first_name\",\n",
        "\"description\": \"Customer's first name\",\n",
        "\n",
        "\"data_type\": \"STRING\",\n",
        "\"format\": \"CamelCase\",\n",
        "\"is_pii_column\": \"Y\",\n",
        "\"enum\": [],\n",
        "\"dimension_group\": \"customer_dimension_group\",\n",
        "\"is_nullable\": \"N\"\n",
        "},\n",
        "{\n",
        "\"name\": \"last_name\",\n",
        "\"description\": \"Customer's last name\",\n",
        "\"data_type\": \"STRING\",\n",
        "\"format\": \"CamelCase\",\n",
        "\"is_pii_column\": \"Y\",\n",
        "\"enum\": [],\n",
        "\"dimension_group\": \"customer_dimension_group\",\n",
        "\"is_nullable\": \"N\"\n",
        "}\n",
        "]\n",
        "},\n",
        "{\n",
        "\"table_name\": \"order\",\n",
        "\"table_description\": \"Stores detailed records of customer orders\",\n",
        "\"primary_key_column\": [\n",
        "\"order_id\",\n",
        "\"order_item_id\"\n",
        "],\n",
        "\"columns\": [\n",
        "{\n",
        "\"name\": \"order_id\",\n",
        "\"description\": \"Unique identifier for the order\",\n",
        "\"data_type\": \"STRING\",\n",
        "\"format\": \"\",\n",
        "\"is_pii_column\": \"N\",\n",
        "\"enum\": [],\n",
        "\"is_nullable\": \"N\"\n",
        "},\n",
        "{\n",
        "\"name\": \"customer_id\",\n",
        "\"description\": \"Unique identifier for the customer\",\n",
        "\"data_type\": \"STRING\",\n",
        "\"format\": \"\",\n",
        "\"is_pii_column\": \"Y\",\n",
        "\"enum\": [],\n",
        "\"is_nullable\": \"N\"\n",
        "}\n",
        "]\n",
        "}\n",
        "\n",
        "]\n",
        "}\n",
        "Relationship\n",
        "\"relationships\": [\n",
        "{\n",
        "\"FromTable\": \"customer_0\",\n",
        "\"FromColumn\": [\n",
        "\"customer_id\"\n",
        "],\n",
        "\"ToTable\": \"order\",\n",
        "\"ToColumn\": [\n",
        "\"customer_id\"\n",
        "]\n",
        "}\n",
        "]\n",
        "}]\n",
        "\n",
        "<|end|>\n",
        "************************************Now************************************\n",
        "User ask  :How can I identify customers from specific regions who have spent more than $500 in the last month and assess their satisfaction scores?\n",
        "Respone : \" \"\n",
        " \"\"\"\n",
        "device = \"cuda:0\"\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "outputs = model.generate(**inputs, max_new_tokens=1000)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=False))"
      ],
      "metadata": {
        "id": "T1TiIH6vAlr_",
        "outputId": "92871384-690a-4883-c509-68501c9169ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> You have to answer the question only in json format.For example if user ask : Give me top 5 customer with most number of orders.\n",
            "Then you have to give response as : \n",
            "[ \"table_information\": [\n",
            "{\n",
            "\"table_name\": \"customer_0\",\n",
            "\"table_description\": \"Stores detailed customer information and behavior\",\n",
            "\"primary_key_column\": [\n",
            "\"customer_id\"\n",
            "],\n",
            "\"columns\": [\n",
            "{\n",
            "\"name\": \"customer_id\",\n",
            "\"description\": \"Unique identifier for the customer\",\n",
            "\"data_type\": \"STRING\",\n",
            "\"format\": \"\",\n",
            "\"is_pii_column\": \"Y\",\n",
            "\"enum\": [],\n",
            "\"dimension_group\": \"\",\n",
            "\"is_nullable\": \"N\"\n",
            "},\n",
            "{\n",
            "\"name\": \"first_name\",\n",
            "\"description\": \"Customer's first name\",\n",
            "\n",
            "\"data_type\": \"STRING\",\n",
            "\"format\": \"CamelCase\",\n",
            "\"is_pii_column\": \"Y\",\n",
            "\"enum\": [],\n",
            "\"dimension_group\": \"customer_dimension_group\",\n",
            "\"is_nullable\": \"N\"\n",
            "},\n",
            "{\n",
            "\"name\": \"last_name\",\n",
            "\"description\": \"Customer's last name\",\n",
            "\"data_type\": \"STRING\",\n",
            "\"format\": \"CamelCase\",\n",
            "\"is_pii_column\": \"Y\",\n",
            "\"enum\": [],\n",
            "\"dimension_group\": \"customer_dimension_group\",\n",
            "\"is_nullable\": \"N\"\n",
            "}\n",
            "]\n",
            "},\n",
            "{\n",
            "\"table_name\": \"order\",\n",
            "\"table_description\": \"Stores detailed records of customer orders\",\n",
            "\"primary_key_column\": [\n",
            "\"order_id\",\n",
            "\"order_item_id\"\n",
            "],\n",
            "\"columns\": [\n",
            "{\n",
            "\"name\": \"order_id\",\n",
            "\"description\": \"Unique identifier for the order\",\n",
            "\"data_type\": \"STRING\",\n",
            "\"format\": \"\",\n",
            "\"is_pii_column\": \"N\",\n",
            "\"enum\": [],\n",
            "\"is_nullable\": \"N\"\n",
            "},\n",
            "{\n",
            "\"name\": \"customer_id\",\n",
            "\"description\": \"Unique identifier for the customer\",\n",
            "\"data_type\": \"STRING\",\n",
            "\"format\": \"\",\n",
            "\"is_pii_column\": \"Y\",\n",
            "\"enum\": [],\n",
            "\"is_nullable\": \"N\"\n",
            "}\n",
            "]\n",
            "}\n",
            "\n",
            "]\n",
            "}\n",
            "Relationship\n",
            "\"relationships\": [\n",
            "{\n",
            "\"FromTable\": \"customer_0\",\n",
            "\"FromColumn\": [\n",
            "\"customer_id\"\n",
            "],\n",
            "\"ToTable\": \"order\",\n",
            "\"ToColumn\": [\n",
            "\"customer_id\"\n",
            "]\n",
            "}\n",
            "]\n",
            "}]\n",
            "\n",
            "<|end|> ************************************Now************************************\n",
            "User ask  :How can I identify customers from specific regions who have spent more than $500 in the last month and assess their satisfaction scores?\n",
            "Respone : \" \" \n",
            " \n",
            "<|assistant|> [\n",
            "{\n",
            "\"table_information\": [\n",
            "{\n",
            "\"table_name\": \"customer\",\n",
            "\"table_description\": \"Stores detailed customer information and behavior\",\n",
            "\"primary_key_column\": [\n",
            "\"customer_id\"\n",
            "],\n",
            "\"columns\": [\n",
            "{\n",
            "\"name\": \"customer_id\",\n",
            "\"description\": \"Unique identifier for the customer\",\n",
            "\"data_type\": \"STRING\",\n",
            "\"format\": \"\",\n",
            "\"is_pii_column\": \"Y\",\n",
            "\"enum\": [],\n",
            "\"dimension_group\": \"customer_dimension_group\",\n",
            "\"is_nullable\": \"N\"\n",
            "},\n",
            "{\n",
            "\"name\": \"region\",\n",
            "\"description\": \"Region where the customer resides\",\n",
            "\"data_type\": \"STRING\",\n",
            "\"format\": \"\",\n",
            "\"is_pii_column\": \"Y\",\n",
            "\"enum\": [],\n",
            "\"dimension_group\": \"customer_dimension_group\",\n",
            "\"is_nullable\": \"N\"\n",
            "},\n",
            "{\n",
            "\"name\": \"total_spend\",\n",
            "\"description\": \"Total amount spent by the customer in the last month\",\n",
            "\"data_type\": \"FLOAT\",\n",
            "\"format\": \"\",\n",
            "\"is_pii_column\": \"Y\",\n",
            "\"enum\": [],\n",
            "\"dimension_group\": \"customer_dimension_group\",\n",
            "\"is_nullable\": \"N\"\n",
            "},\n",
            "{\n",
            "\"name\": \"satisfaction_score\",\n",
            "\"description\": \"Customer's satisfaction score\",\n",
            "\"data_type\": \"FLOAT\",\n",
            "\"format\": \"\",\n",
            "\"is_pii_column\": \"Y\",\n",
            "\"enum\": [],\n",
            "\"dimension_group\": \"customer_dimension_group\",\n",
            "\"is_nullable\": \"N\"\n",
            "}\n",
            "]\n",
            "},\n",
            "{\n",
            "\"table_name\": \"order\",\n",
            "\"table_description\": \"Stores detailed records of customer orders\",\n",
            "\"primary_key_column\": [\n",
            "\"order_id\",\n",
            "\"order_item_id\"\n",
            "],\n",
            "\"columns\": [\n",
            "{\n",
            "\"name\": \"order_id\",\n",
            "\"description\": \"Unique identifier for the order\",\n",
            "\"data_type\": \"STRING\",\n",
            "\"format\": \"\",\n",
            "\"is_pii_column\": \"N\",\n",
            "\"enum\": [],\n",
            "\"is_nullable\": \"N\"\n",
            "},\n",
            "{\n",
            "\"name\": \"customer_id\",\n",
            "\"description\": \"Unique identifier for the customer\",\n",
            "\"data_type\": \"STRING\",\n",
            "\"format\": \"\",\n",
            "\"is_pii_column\": \"Y\",\n",
            "\"enum\": [],\n",
            "\"is_nullable\": \"N\"\n",
            "}\n",
            "]\n",
            "},\n",
            "{\n",
            "\"table_name\": \"order_item\",\n",
            "\"table_description\": \"Stores detailed records of items in customer orders\",\n",
            "\"primary_key_column\": [\n",
            "\"order_item_id\"\n",
            "],\n",
            "\"columns\": [\n",
            "{\n",
            "\"name\": \"order_item_id\",\n",
            "\"description\": \"Unique identifier for the order item\",\n",
            "\"data_type\": \"STRING\",\n",
            "\"format\": \"\",\n",
            "\"is_pii_column\": \"N\",\n",
            "\"enum\": [],\n",
            "\"is_nullable\": \"N\"\n",
            "},\n",
            "{\n",
            "\"name\": \"price\",\n",
            "\"description\": \"Price of the item\",\n",
            "\"data_type\": \"FLOAT\",\n",
            "\"format\": \"\",\n",
            "\"is_pii_column\": \"N\",\n",
            "\"enum\": [],\n",
            "\"is_nullable\": \"N\"\n",
            "}\n",
            "]\n",
            "}\n",
            "]\n",
            "\n",
            "[\n",
            "{\n",
            "\"relationships\": [\n",
            "{\n",
            "\"FromTable\": \"customer\",\n",
            "\"FromColumn\": [\n",
            "\"customer_id\"\n",
            "],\n",
            "\"ToTable\": \"order\",\n",
            "\"ToColumn\": [\n",
            "\"customer_id\"\n",
            "]\n",
            "},\n",
            "{\n",
            "\"FromTable\": \"customer\",\n",
            "\"FromColumn\": [\n",
            "\"customer_id\"\n",
            "],\n",
            "\"ToTable\": \"order_item\",\n",
            "\"ToColumn\": [\n",
            "\"order_item_id\"\n",
            "]\n",
            "}\n",
            "]\n",
            "}\n",
            "]\n",
            "}\n",
            "\n",
            "To answer the user's question, we need to join the \"customer\", \"order\", and \"order_item\" tables based on the relationships defined above. Then, we can filter the customers based on their region, total spend in the last month, and satisfaction score. Here is the JSON response:\n",
            "\n",
            "[\n",
            "{\n",
            "\"query\": \"SELECT c.customer_id, c.region, c.total_spend, c.satisfaction_score\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"lora_model\") # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ],
      "metadata": {
        "id": "mmxQjVO8hYd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TJ8ttWvHhp13"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Output 1"
      ],
      "metadata": {
        "id": "Yekff6mjg43n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass"
      ],
      "metadata": {
        "id": "nb8v1YATExu1"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# alpaca_prompt = Copied from above\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"\"\"Use only the table and relationship between table exist. Don't create any other new table and use logic to give answer and response is in json format.\n",
        "        For example :\n",
        "Question : /n\n",
        "Give me top 5 customer with most number of orders.\n",
        "then/n\n",
        "Expected Output :\n",
        "Columns\n",
        "\n",
        "{\n",
        "\"table_information\": [\n",
        "{\n",
        "\"table_name\": \"customer_0\",\n",
        "\"table_description\": \"Stores detailed customer information and behavior\",\n",
        "\"primary_key_column\": [\n",
        "\"customer_id\"\n",
        "],\n",
        "\"columns\": [\n",
        "{\n",
        "\"name\": \"customer_id\",\n",
        "\"description\": \"Unique identifier for the customer\",\n",
        "\"data_type\": \"STRING\",\n",
        "\"format\": \"\",\n",
        "\"is_pii_column\": \"Y\",\n",
        "\"enum\": [],\n",
        "\"dimension_group\": \"\",\n",
        "\"is_nullable\": \"N\"\n",
        "},\n",
        "{\n",
        "\"name\": \"first_name\",\n",
        "\"description\": \"Customer's first name\",\n",
        "\n",
        "\"data_type\": \"STRING\",\n",
        "\"format\": \"CamelCase\",\n",
        "\"is_pii_column\": \"Y\",\n",
        "\"enum\": [],\n",
        "\"dimension_group\": \"customer_dimension_group\",\n",
        "\"is_nullable\": \"N\"\n",
        "},\n",
        "{\n",
        "\"name\": \"last_name\",\n",
        "\"description\": \"Customer's last name\",\n",
        "\"data_type\": \"STRING\",\n",
        "\"format\": \"CamelCase\",\n",
        "\"is_pii_column\": \"Y\",\n",
        "\"enum\": [],\n",
        "\"dimension_group\": \"customer_dimension_group\",\n",
        "\"is_nullable\": \"N\"\n",
        "}\n",
        "]\n",
        "},\n",
        "{\n",
        "\"table_name\": \"order\",\n",
        "\"table_description\": \"Stores detailed records of customer orders\",\n",
        "\"primary_key_column\": [\n",
        "\"order_id\",\n",
        "\"order_item_id\"\n",
        "],\n",
        "\"columns\": [\n",
        "{\n",
        "\"name\": \"order_id\",\n",
        "\"description\": \"Unique identifier for the order\",\n",
        "\"data_type\": \"STRING\",\n",
        "\"format\": \"\",\n",
        "\"is_pii_column\": \"N\",\n",
        "\"enum\": [],\n",
        "\"is_nullable\": \"N\"\n",
        "},\n",
        "{\n",
        "\"name\": \"customer_id\",\n",
        "\"description\": \"Unique identifier for the customer\",\n",
        "\"data_type\": \"STRING\",\n",
        "\"format\": \"\",\n",
        "\"is_pii_column\": \"Y\",\n",
        "\"enum\": [],\n",
        "\"is_nullable\": \"N\"\n",
        "}\n",
        "]\n",
        "}\n",
        "\n",
        "]\n",
        "}\n",
        "Relationship\n",
        "\"relationships\": [\n",
        "{\n",
        "\"FromTable\": \"customer_0\",\n",
        "\"FromColumn\": [\n",
        "\"customer_id\"\n",
        "],\n",
        "\"ToTable\": \"order\",\n",
        "\"ToColumn\": [\n",
        "\"customer_id\"\n",
        "]\n",
        "}\n",
        "]\n",
        "}\n",
        "\"\"\", # instruction\n",
        "        \"\"\" Question:  What is the average duration from order placement to shipment across different product categories, segmented by customer regions and order volume? \"\"\", # input\n",
        "        \"  \", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "# outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
        "# tokenizer.batch_decode(outputs)"
      ],
      "metadata": {
        "id": "ykcfgfVAEorZ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# alpaca_prompt = Copied from above\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer,max_new_tokens = 400,use_cache = False)"
      ],
      "metadata": {
        "id": "gYRCn5k3E3oG",
        "outputId": "81523a3b-613c-4fc2-dd5c-06b32eb7eff8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Use only the table and relationship between table exist. Don't create any other new table and use logic to give answer and response is in json format.\n",
            "        For example : \n",
            "Question : /n\n",
            "Give me top 5 customer with most number of orders.\n",
            "then/n\n",
            "Expected Output :\n",
            "Columns\n",
            "\n",
            "{\n",
            "\"table_information\": [\n",
            "{\n",
            "\"table_name\": \"customer_0\",\n",
            "\"table_description\": \"Stores detailed customer information and behavior\",\n",
            "\"primary_key_column\": [\n",
            "\"customer_id\"\n",
            "],\n",
            "\"columns\": [\n",
            "{\n",
            "\"name\": \"customer_id\",\n",
            "\"description\": \"Unique identifier for the customer\",\n",
            "\"data_type\": \"STRING\",\n",
            "\"format\": \"\",\n",
            "\"is_pii_column\": \"Y\",\n",
            "\"enum\": [],\n",
            "\"dimension_group\": \"\",\n",
            "\"is_nullable\": \"N\"\n",
            "},\n",
            "{\n",
            "\"name\": \"first_name\",\n",
            "\"description\": \"Customer's first name\",\n",
            "\n",
            "\"data_type\": \"STRING\",\n",
            "\"format\": \"CamelCase\",\n",
            "\"is_pii_column\": \"Y\",\n",
            "\"enum\": [],\n",
            "\"dimension_group\": \"customer_dimension_group\",\n",
            "\"is_nullable\": \"N\"\n",
            "},\n",
            "{\n",
            "\"name\": \"last_name\",\n",
            "\"description\": \"Customer's last name\",\n",
            "\"data_type\": \"STRING\",\n",
            "\"format\": \"CamelCase\",\n",
            "\"is_pii_column\": \"Y\",\n",
            "\"enum\": [],\n",
            "\"dimension_group\": \"customer_dimension_group\",\n",
            "\"is_nullable\": \"N\"\n",
            "}\n",
            "]\n",
            "},\n",
            "{\n",
            "\"table_name\": \"order\",\n",
            "\"table_description\": \"Stores detailed records of customer orders\",\n",
            "\"primary_key_column\": [\n",
            "\"order_id\",\n",
            "\"order_item_id\"\n",
            "],\n",
            "\"columns\": [\n",
            "{\n",
            "\"name\": \"order_id\",\n",
            "\"description\": \"Unique identifier for the order\",\n",
            "\"data_type\": \"STRING\",\n",
            "\"format\": \"\",\n",
            "\"is_pii_column\": \"N\",\n",
            "\"enum\": [],\n",
            "\"is_nullable\": \"N\"\n",
            "},\n",
            "{\n",
            "\"name\": \"customer_id\",\n",
            "\"description\": \"Unique identifier for the customer\",\n",
            "\"data_type\": \"STRING\",\n",
            "\"format\": \"\",\n",
            "\"is_pii_column\": \"Y\",\n",
            "\"enum\": [],\n",
            "\"is_nullable\": \"N\"\n",
            "}\n",
            "]\n",
            "}\n",
            "\n",
            "]\n",
            "}\n",
            "Relationship\n",
            "\"relationships\": [\n",
            "{\n",
            "\"FromTable\": \"customer_0\",\n",
            "\"FromColumn\": [\n",
            "\"customer_id\"\n",
            "],\n",
            "\"ToTable\": \"order\",\n",
            "\"ToColumn\": [\n",
            "\"customer_id\"\n",
            "]\n",
            "}\n",
            "]\n",
            "}\n",
            "\n",
            "\n",
            "### Input:\n",
            " Question:  What is the average duration from order placement to shipment across different product categories, segmented by customer regions and order volume? \n",
            "\n",
            "### Response:\n",
            "  \n",
            "{\n",
            " \"table_information\": [\n",
            "   {\n",
            "     \"table_name\": \"order\",\n",
            "     \"table_description\": \"Stores detailed records of customer orders\",\n",
            "     \"primary_key_column\": [\"order_id\", \"order_item_id\"],\n",
            "     \"columns\": [\n",
            "       {\n",
            "         \"name\": \"order_id\",\n",
            "         \"description\": \"Unique identifier for the order\",\n",
            "         \"data_type\": \"STRING\",\n",
            "         \"format\": \"\",\n",
            "         \"is_pii_column\": \"N\",\n",
            "         \"enum\": [],\n",
            "         \"is_nullable\": \"N\"\n",
            "       },\n",
            "       {\n",
            "         \"name\": \"customer_id\",\n",
            "\"description\": \"Unique identifier for the customer\",\n",
            "\"data_type\": \"STRING\",\n",
            "\"format\": \"\",\n",
            "\"is_pii_column\": \"Y\",\n",
            "\"enum\": [],\n",
            "\"is_nullable\": \"N\"\n",
            "   }\n",
            " ]\n",
            "}\n",
            "\n",
            " \"relationships\": [\n",
            "   {\n",
            "     \"FromTable\": \"order\",\n",
            "     \"FromColumn\": [\"customer_id\"],\n",
            "     \"ToTable\": \"customer_0\",\n",
            "     \"ToColumn\": [\"customer_id\"]\n",
            "   }\n",
            " ]\n",
            "}\n",
            "\n",
            "\n",
            "### Response:\n",
            "\n",
            "{\n",
            " \"table_information\": [\n",
            "   {\n",
            "     \"table_name\": \"customer_0\",\n",
            "     \"table_description\": \"Stores detailed customer information and behavior\",\n",
            "     \"primary_key_column\": [\"customer_id\"],\n",
            "     \"columns\": [\n",
            "      "
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-9576fafae8fc>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextStreamer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtext_streamer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextStreamer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstreamer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_streamer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_new_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muse_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/unsloth/models/llama.py\u001b[0m in \u001b[0;36m_fast_generate\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;31m# Autocasted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/unsloth/models/llama.py\u001b[0m in \u001b[0;36m_fast_generate\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;31m# Autocasted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/unsloth/models/llama.py\u001b[0m in \u001b[0;36m_fast_generate\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;31m# Autocasted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1489\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m                     \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1491\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1492\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1574\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgeneration_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mGenerationMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGREEDY_SEARCH\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1575\u001b[0m             \u001b[0;31m# 11. run greedy search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1576\u001b[0;31m             result = self._greedy_search(\n\u001b[0m\u001b[1;32m   1577\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1578\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2539\u001b[0m             \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2540\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstreamer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2541\u001b[0;31m                 \u001b[0mstreamer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2542\u001b[0m             model_kwargs = self._update_model_kwargs_for_generation(\n\u001b[1;32m   2543\u001b[0m                 \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Output 2"
      ],
      "metadata": {
        "id": "xrsTU0ktg9CO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "\n",
        "    {\"role\": \"user\", \"content\": \"\"\"Give me top 5 customer with most number of orders.\"\"\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"\"\"Sure! Here expected output : [ \"table_information\": [\n",
        "{\n",
        "\"table_name\": \"customer_0\",\n",
        "\"table_description\": \"Stores detailed customer information and behavior\",\n",
        "\"primary_key_column\": [\n",
        "\"customer_id\"\n",
        "],\n",
        "\"columns\": [\n",
        "{\n",
        "\"name\": \"customer_id\",\n",
        "\"description\": \"Unique identifier for the customer\",\n",
        "\"data_type\": \"STRING\",\n",
        "\"format\": \"\",\n",
        "\"is_pii_column\": \"Y\",\n",
        "\"enum\": [],\n",
        "\"dimension_group\": \"\",\n",
        "\"is_nullable\": \"N\"\n",
        "},\n",
        "{\n",
        "\"name\": \"first_name\",\n",
        "\"description\": \"Customer's first name\",\n",
        "\n",
        "\"data_type\": \"STRING\",\n",
        "\"format\": \"CamelCase\",\n",
        "\"is_pii_column\": \"Y\",\n",
        "\"enum\": [],\n",
        "\"dimension_group\": \"customer_dimension_group\",\n",
        "\"is_nullable\": \"N\"\n",
        "},\n",
        "{\n",
        "\"name\": \"last_name\",\n",
        "\"description\": \"Customer's last name\",\n",
        "\"data_type\": \"STRING\",\n",
        "\"format\": \"CamelCase\",\n",
        "\"is_pii_column\": \"Y\",\n",
        "\"enum\": [],\n",
        "\"dimension_group\": \"customer_dimension_group\",\n",
        "\"is_nullable\": \"N\"\n",
        "}\n",
        "]\n",
        "},\n",
        "{\n",
        "\"table_name\": \"order\",\n",
        "\"table_description\": \"Stores detailed records of customer orders\",\n",
        "\"primary_key_column\": [\n",
        "\"order_id\",\n",
        "\"order_item_id\"\n",
        "],\n",
        "\"columns\": [\n",
        "{\n",
        "\"name\": \"order_id\",\n",
        "\"description\": \"Unique identifier for the order\",\n",
        "\"data_type\": \"STRING\",\n",
        "\"format\": \"\",\n",
        "\"is_pii_column\": \"N\",\n",
        "\"enum\": [],\n",
        "\"is_nullable\": \"N\"\n",
        "},\n",
        "{\n",
        "\"name\": \"customer_id\",\n",
        "\"description\": \"Unique identifier for the customer\",\n",
        "\"data_type\": \"STRING\",\n",
        "\"format\": \"\",\n",
        "\"is_pii_column\": \"Y\",\n",
        "\"enum\": [],\n",
        "\"is_nullable\": \"N\"\n",
        "}\n",
        "]\n",
        "}\n",
        "\n",
        "]\n",
        "}\n",
        "Relationship\n",
        "\"relationships\": [\n",
        "{\n",
        "\"FromTable\": \"customer_0\",\n",
        "\"FromColumn\": [\n",
        "\"customer_id\"\n",
        "],\n",
        "\"ToTable\": \"order\",\n",
        "\"ToColumn\": [\n",
        "\"customer_id\"\n",
        "]\n",
        "}\n",
        "]\n",
        "}]\"\"\"},\n",
        "    {\"role\": \"user\", \"content\": \"\"\"What is the average duration from order placement to shipment across different product categories, segmented by customer regions and order volume?\"\"\"},\n",
        "]\n"
      ],
      "metadata": {
        "id": "qW_vPxaFNCKG"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "generation_args = {\n",
        "    \"max_new_tokens\": 1000,\n",
        "    \"return_full_text\": False,\n",
        "    \"temperature\": 1 ,\n",
        "    \"do_sample\": False,\n",
        "}\n",
        "\n",
        "output = pipe(messages, **generation_args)\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "vqFnWZabLByG",
        "outputId": "4810e12b-6841-423d-9c9d-19c964520203",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " To answer this question, we need to perform a multi-dimensional analysis on the database. Here is a step-by-step approach:\n",
            "\n",
            "1. Join the \"order\" table with the \"product\" table on the product_id column to get the product category.\n",
            "2. Join the \"order\" table with the \"customer\" table on the customer_id column to get the customer region.\n",
            "3. Calculate the duration from order placement to shipment for each order.\n",
            "4. Group the data by product category, customer region, and order volume.\n",
            "5. Calculate the average duration for each group.\n",
            "\n",
            "Here is a SQL query that accomplishes this:\n",
            "\n",
            "```sql\n",
            "SELECT \n",
            "    p.category AS product_category,\n",
            "    c.region AS customer_region,\n",
            "    CASE \n",
            "        WHEN o.quantity <= 10 THEN 'low'\n",
            "        WHEN o.quantity <= 100 THEN'medium'\n",
            "        ELSE 'high'\n",
            "    END AS order_volume,\n",
            "    AVG(o.shipment_date - o.order_date) AS avg_duration\n",
            "FROM \n",
            "    order o\n",
            "    JOIN product p ON o.product_id = p.product_id\n",
            "    JOIN customer c ON o.customer_id = c.customer_id\n",
            "GROUP BY _\n",
            "    product_category,\n",
            "    customer_region,\n",
            "    order_volume\n",
            "```\n",
            "\n",
            "This query will provide the average duration from order placement to shipment across different product categories, segmented by customer regions and order volume. To further analyze the data, you can also calculate the median and standard deviation of the duration for each group. This will give you a better understanding of the distribution of the duration.\n",
            "\n",
            "Here is an extended SQL query that calculates the median and standard deviation:\n",
            "\n",
            "```sql\n",
            "SELECT \n",
            "    product_category,\n",
            "    customer_region,\n",
            "    order_volume,\n",
            "    AVG(duration) AS avg_duration,\n",
            "    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY duration) AS median_duration,\n",
            "    STDDEV(duration) AS std_dev_duration\n",
            "FROM (\n",
            "    SELECT \n",
            "        p.category AS product_category,\n",
            "        c.region AS customer_region,\n",
            "        CASE \n",
            "            WHEN o.quantity <= 10 THEN 'low'\n",
            "            WHEN o.quantity <= 100 THEN'medium'\n",
            "            ELSE 'high'\n",
            "        END AS order_volume,\n",
            "        (o.shipment_date - o.order_date) AS duration\n",
            "    FROM \n",
            "        order o\n",
            "        JOIN product p ON o.product_id = p.product_id\n",
            "        JOIN customer c ON o.customer_id = c.customer_id\n",
            ") subquery\n",
            "GROUP BY \n",
            "    product_category,\n",
            "    customer_region,\n",
            "    order_volume\n",
            "```\n",
            "\n",
            "This query will provide the average, median, and standard deviation of the duration from order placement to shipment across different product categories, segmented by customer regions and order volume. Additionally, you can visualize the data using a heatmap or a multi-dimensional bubble chart to better understand the patterns and relationships between the variables.\n",
            "\n",
            "For example, you can create a heatmap using the following Python code with the seaborn library:\n",
            "\n",
            "```python\n",
            "import seaborn as sns\n",
            "import pandas as pd\n",
            "\n",
            "# Assuming the result of the SQL query is stored in a pandas DataFrame called df\n",
            "df = pd.read_sql_query(query, connection)\n",
            "\n",
            "# Pivot the table to get the desired format for the heatmap\n",
            "df_pivot = df.pivot_table(index=['product_category', 'customer_region', 'order_volume'], \n",
            "                           values=['avg_duration','median_duration','std_dev_duration'],\n",
            "                           aggfunc='first')\n",
            "\n",
            "# Create the heatmap\n",
            "heatmap_data = df_pivot.unstack().reset_index()\n",
            "heatmap_data.columns = ['product_category', 'customer_region', 'order_volume', 'avg_duration','median_duration','std_dev_duration']\n",
            "heatmap_data = heatmap_data.fillna(0)\n",
            "\n",
            "sns.heatmap(heatmap_data, cmap=\"YlGnBu\")\n",
            "```\n",
            "\n",
            "This heatmap will help you visualize the average, median, and standard deviation of the duration from order placement to shipment across different\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save model"
      ],
      "metadata": {
        "id": "ih2wbJ7QhXLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"lora_model\") # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ],
      "metadata": {
        "collapsed": true,
        "id": "NrqSkeO3hrvj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}